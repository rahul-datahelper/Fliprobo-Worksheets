WEB SCRAPING
WORKSHEET – 1

In Q1 to Q9, only one option is correct, Choose the correct option:
1.	Which of the following extracts information from user generated content?
B) Web scraping

2.	Which of the following is not a web scraping library in python?
A) selenium		

3.Selenium tests __________?
A) Browser based applications

4.	Task of crawling is performed by a complex software which is known as:
B) Crawler

5.	Which of the following commands is used to access name of a tag in Beautiful Soup?
B) tag.name

6.	Which of the following is the default parser in Beautiful Soup?
C) lxml

7.	In selenium the webdriver is used to?
C) execute tests on HtmlUnit browser

8.	In selenium, driver.find_elements_by_xpath(‘given xpath’) returns:
C) the list of all webelements associated with the ‘given xpath’

9.	The script ‘window.scrollBy(0,a) scrolls the webpage by?
D) ‘a’ number of pixels vertically

In Q10, more than one options are correct, Choose all the correct options:
10.	Which of the following is(are) tags of HTML?
A) <a>					B) <b>			D) <href>

Q10 to Q13 are subjective answer type questions, Answer them briefly.
11.	What is the main difference between a web scraper and a web crawler?

Web crawling is perform by search engines. It is all about viewing a page as a whole and indexing it. 
When a bot crawls a website, it goes through every page and every link, until the last line of the website, 
looking for each information. Search engines use web crawling for the purpose of SEO, SMO. 
It crawls all data that are present over a web. The crawler also known as spider or bot.

Web scrapping involves specific data extraction on a targeted webpage, for instance, extract data about sales leads, 
real estate listing and product pricing. It is more focused and intentional and usually is directed toward a website to scrap. 
It’s the programmatic analysis of a web page to download information from it. By using various scrapping tools we can scrap images, videos, text etc, from any site.


12.	What is ‘robots.txt’ file? What is the use of ‘robots.txt’ file?
A robots.txt is a file with set of instructions for bots or crawlers. This file is included in the source file of websites. 
It is a text file that tells which parts of website can be crawled and which part cannot be, it also includes a link to sitemap 
(can consider as directory of website) of the website. 
By using a robots.txt individual files in a directories, complete directories, sub directories can be excluded from crawling.
 The robots.txt data is stored in the root of the domain. It is the first document that is accessed by a bot when it visits a website. 
The bots of the biggest search engines such as Google and Bing follow the instructions. Otherwise there is no guarantee that a bot will 
adhere to the robots.txt requirements. In short, Robots.txt helps to control the crawling of search engine bots. Robots.txt is very helpful for SEO purpose.


13.	What are static and dynamic web pages?
Static website is using pure HTML, CSS, JS without relying on the content, or parts of the content, being pulled from various databases.
 This means each page is constructed on its own, coded, and loaded individually, no matter who or how the visitor arrives on the page.
 In Static website content will not change until someone change it manually. Static website is very easy to manage and have the low maintenance cost.

Q14 and Q15 are programming practice questions. Solve it using JUPYTER NOTEBOOK and paste the solution in your answer sheets.
14.	Write a python program to check whether a webpage contains a title or not.
https://github.com/rahul-datahelper/Fliprobo-Worksheets/blob/master/check%20title.ipynb


15.	Write a python program to access the search bar and search button on images.google.com.
https://github.com/rahul-datahelper/Fliprobo-Worksheets/blob/master/Access%20image%20from%20google.ipynb